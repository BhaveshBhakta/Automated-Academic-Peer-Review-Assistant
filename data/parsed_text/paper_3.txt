arXiv:1707.04849v1  [cs.LG]  16 Jul 2017
Minimax deviation strategies for machine
learning and recognition with short learning
samples
Schlesinger M.I. and Vodolazskiy E.V.
August 31, 2018
Abstract
The article is devoted to the problem of small learning samples in
machine learning. The ﬂaws of maximum likelihood learning and min-
imax learning are looked into and the concept of minimax deviation
learning is introduced that is free of those ﬂaws.
1
Introduction
The small learning sample problem has been around in machine learning
under diﬀerent names during its whole life. The learning sample is used to
compensate for the lack of knowledge about the recognized object when its
statistical model is not completely known. Naturally, the longer the learning
sample is, the better is the subsequent recognition. However, when the learn-
ing sample becomes too small (2, 3, 5 elements) an eﬀect of small samples
becomes evident. In spite of the fact that any learning sample (even a very
small one) provides some additional information about the object, it may be
better to ignore the learning sample than to utilize it with the commonly
used methods.
Example 1. Let us consider an object that can be in one of two random
states y = 1 and y = 2 with equal probabilities. In each state the object
generates two independent Gaussian random signals x1 and x2 with variances
equal 1. Mean values of signals depend on the state as it is shown on Fig.
1

1. In the ﬁrst state the mean value is (2, 0). In the second state the mean
value depends on an unknown parameter θ and is (0, θ). Even if no learning
sample is given a minimax strategy can be used to make a decision about the
state y. The minimax strategy ignores the second signal and makes decision
y∗= 1 when x1 > 1 and decision y∗= 2 when x1 ≤1.
x2
x1
b
b
θ
b
2
0
y∗= 1
y∗= 2
p(x1, x2|y = 1)
p(x1, x2|y = 2)
Figure 1: Example 1. (x1, x2) ∈R2 – signal, y ∈{1, 2} – state.
Now let us assume that there is a sample of signals generated by an
object in the second state but with higher variance 16. A maximum likelihood
strategy estimates the unknown parameter θ and then makes a decision about
y as if the estimated value of the parameter is its true value. Fig. 2 shows how
the probability of a wrong decision (called the risk) depends on parameter θ
for diﬀerent sizes of the learning sample. If the learning sample is suﬃciently
long, the risk of maximum likelihood strategy may become arbitrarily close
to the minimum possible risk.
Naturally, when the length of the sample
decreases the risk becomes worse and worse. Furthermore, when it becomes
as small as 3 or 2 elements the risk of the maximum likelihood strategy
becomes worse than the risk of the minimax strategy that uses neither the
learning sample nor the signal x2 at all. Hence, it is better to ignore available
additional data about the recognized object than to try to make use of it in
a conventional way. It demonstrates a serious theoretical ﬂaw of commonly
used methods, and deﬁnitely not that short samples are useless. Any learning
sample, no mater how long or short it is, provides some, may be not a lot
information about the recognized object and a reasonable method has to use
it.
2

θ
b
−6
−3
−0
3
6
b
b
b
b
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
θ
b
−6
−3
−0
3
6
b
b
b
b
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
n = 1
n = 2
θ
b
−6
−3
−0
3
6
b
b
b
b
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
θ
b
−6
−3
−0
3
6
b
b
b
b
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
n = 3
n = 10
Figure 2: Example 1. Probability of a wrong decision (risk) for diﬀerent
sizes n of the learning sample. The curve R(qML, θ) is the risk of a maximum
likelihood strategy. The curve R(qminmax, θ) is the risk of a minimax strategy.
The curve min
q
R(q, θ) is the minimum possible risk for each model.
Example 2. This is a simple example that has been used by H.Robbins in
his seminal article [5] where he initiated empirical Bayessian approach and
explaned its main idea. An object can be in one of two possible states y = 1
and y = 2. In each state the object generates a univariate Gaussian signal
x with variance 1. The mean value of the generated signal depends on the
state y so that
p(x|y = 1) =
1
√
2πe−(x+1)2
2
,
p(x|y = 2) =
1
√
2πe−(x−1)2
2
.
Only a priori probabilities of states are unknown and θ is the probability
of the ﬁrst state so that p(y = 1) = θ and p(y = 2) = 1 −θ. A minimax
strategy for such incomplete statistical model makes decision y∗based on
the sign of the observed signal and ensures probability of correct recognition
0.84 independently of a priori probabilities of states.
Let not only a single object, but a collection of mutually independant
objects be available for recognition. Each object is in its own hidden state and
is presented with its own signal. Let us also assume that the decision about
3

x
y∗= 2
y∗= 1
p(x|y = 1)
p(x|y = 2)
b
1
b
−1
b
Figure 3: Example 2. x ∈R – signal, y ∈{1, 2} – state.
each object’s state does not have to be made immediately when the object
is observed and can be postponed until the whole collection is observed. In
this case maximum likelihood estimations of a priori probabilities of states
can be computed and then each object of the collection is recognized as if the
estimated values of probabilities were the true values. When the presented
collection is suﬃciently long the probability of a wrong decision can be made
as close to the minimum as possible (Fig.4). However, when the collection is
too short, the probability of a wrong decision can be much worse than that
of the minimax strategy.
The considered examples lead to a diﬃcult and up to now an unanswered
question. What should be done when a ﬁxed sample of 2-3 elements is given
and no additional elements can be obtained? Is it really the best way to
ignore these data or is it possible to make use of them? We want to ﬁll up
this gap between maximum likelihood and minimax strategies and develop
a strategy that covers teh whole range of learning samples lengths including
zero length. However, this gap, and it is infact a gap, shows a theoretical
imperfection of the commonly used learning procedures, namely, of maximum
likelihood learning. The short sample problem in whole follows from the fact
that maximum likelihood learning as well as many other learning procedures
have not been deduced from any explicit risk-oriented requirement to the
quality of post-learning recognition.
We will formulate such risk-oriented
requirements a priori and will see what type of learning procedures follow.
2
Basic deﬁnitions
Deﬁnition 1. An object is represented with a tuple

X, Y, Θ, pXY : X × Y × Θ →R

4

θ
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
b0
b0.5
b1
θ
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
b0
b0.5
b1
n = 1
n = 2
θ
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
b0
b0.5
b1
θ
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
b0
b0.5
b1
n = 5
n = 10
Figure 4: Example 2. Probability of a wrong decision (risk) for diﬀerent
sizes n of the learning sample.
The curve R(qML, θ) shows the risk of a
maximum likelihood strategy, R(qminmax, θ) is the risk of a minimax strategy,
min
q
R(q, θ) is the minimal possible risk.
where
X is a ﬁnite set of signal values x ∈X;
Y is a ﬁnite set of states y ∈Y ;
Θ is a ﬁnite set of models θ ∈Θ;
pXY (x, y; θ) is a probability of a pair (x ∈X, y ∈Y ) for a model θ ∈Θ.
A signal x is an observable parameter of recognized object whereas a
state y is its hidden parameter. A pair (x, y) is random and for each pair
(x ∈X, y ∈Y ) its probability pXY (x, y; θ) exists. However, this probability
is not known because it depends on an unknown model θ. As for the model
θ it is not random, it takes a ﬁxed but unknown value. Only the set Θ is
known that the value θ belongs to.
Let z be some random data that depend on a model θ and take values
from a ﬁnite set Z. The data is speciﬁed with a tuple

Z, pZ : Z × Θ →R

where pZ(z; θ) is a probability of data z ∈Z for model θ ∈Θ.
5

Deﬁnition 2. A random data

Z, pZ : Z ×Θ →R

that depends on a model
is called a learning data for an object

X, Y, Θ, pXY : X × Y × Θ →R

if
pXY Z(x, y, z; θ) = pXY (x, y; θ) · pZ(z; θ) for all x ∈X, y ∈Y, z ∈Z, θ ∈Θ.
A learning sample ((xi, yi)|i = 1, 2, . . . , n) used for supervised learning is
a special cases of learning data when
Z = (X × Y )n and pZ(z; θ) =
n
Y
i=1
pXY (xi, yi; θ).
A learning sample (xi|i = 1, 2, . . . , n) for unsupervised learning is another
special case of learning data when
Z = Xn and pZ(z; θ) =
n
Y
i=1
X
y∈Y
pXY (xi, y; θ).
Any expert knowledge about the true model is also learning data. One can
even consider the case when |Z| = 1 and therefore pZ(z; θ) = 1, which is
equivalent to the absence of any learning data at all. We do not restrict the
learning data in any way except that for any ﬁxed model the learning data
z depend neither on the current signal x nor on the current state y so that
pXY Z(x, y, z; θ) = pXY (x, y; θ) · pZ(z; θ) for all x ∈X, y ∈Y, z ∈Z, θ ∈Θ.
Deﬁnition 3. A non-negative function q : X×Y ×Z →R is called a strategy
if P
y∈Y q(y|x, z) = 1 for all x ∈X, z ∈Z.
Value q(y|x, z) of a strategy q : X × Y × Z →R is a probability of a
randomized decision that the current state of an object is y, given the current
observed signal x and the available learning data z. The set of all strategies
q : X × Y × Z →R is denoted Q.
Let ω : Y × Y be a loss function whose value ω(y, y′) is the loss of a
decision y′ when the true state is y.
Deﬁnition 4. Risk R(q, θ) of a strategy q on a model θ is expected loss
R(q, θ) =
X
z∈Z
X
x∈X
X
y∈Y
pXY (x, y; θ)pZ(z; θ)
X
y′∈Y
q(y′|x, z)ω(y, y′).
6

Let us be reminded that throughout the paper the sets X, Y , Z and Θ
are assumed to be ﬁnite. This allows a much more transparent formulation
of main results. Allowing some of the sets to be inﬁnite would require ﬁner
mathematical tools and the results might be obscured by unnecessary tech-
nical details.
3
Improper and Bayesian strategies.
One can see that the risk of a strategy depends not only on the strategy itself
but also on the model that the strategy is applied to. Therefore, in a general
case it is not possible to prefer some strategy q1 to another strategy q2. The
risk of q1 may be better than the risk of q2 on some models and worse on the
others. However, it is possible to prefer strategy q2 to strategy q1 if the risk
of q1 is greater than the risk of q2 on all models. In this case we will say that
q2 dominates q1 and q1 is dominated by q2.
Deﬁnition 5. A strategy q0 is called improper if a strategy q∗exists such
that R(q0, θ) > R(q∗, θ)
for all θ ∈Θ.
We want to exclude all improper from consideration strategies and derive
a common form of all the rest.
Let T denote the set of all non-negative
functions τ : Θ →R such that P
θ∈Θ
τ(θ) = 1. Functions of such type will be
reﬀerred to as weight functions.
Deﬁnition 6. A strategy q∗is called Bayesian if there exists a weight func-
tion τ ∈T such that
q∗= arg min
q∈Q
X
θ∈Θ
τ(θ)R(q, θ).
Theorem 1. Each strategy q0 ∈Q is either Bayesian or improper, but never
both.
Proof. For a given strategy q0 let us deﬁne a function F: T × Q →R,
F(τ, q) =
X
θ∈Θ
τ(θ)

R(q, θ) −R(q0, θ)

.
7

According to Deﬁnition 4, for any ﬁxed θ the risk R(q, θ) is a linear function
of probabilities q(y|x, z). Consequently, for any ﬁxed τ the function F is also
a linear function of probabilities q(y|x, z). Similarly, function F is a linear
function of weights τ(θ) for any ﬁxed strategy q. The set Q of strategies
and the set T of weight functions are both closed convex sets. Consequently,
due to the known duality theorem [1, 2, 4] function F has a saddle point
(τ ∗∈T, q∗∈Q) such that
max
τ∈T min
q∈Q F(τ, q) = F(τ ∗, q∗) = min
q∈Q max
τ∈T F(τ, q),
where
q∗= argmin
q∈Q
max
τ∈T F(τ, q),
τ ∗= argmax
τ∈T
min
q∈Q F(τ, q).
It is obvious that F(τ, q0) = 0 for any τ ∈T. Therefore, the inequality
min
q∈Q F(τ, q) ≤0 holds for every τ ∈T and, consequently,
max
τ∈T min
q∈Q F(τ, q) = F(τ ∗, q∗) ≤0.
Therefore, there are two mutually exclusive cases: either F(τ ∗, q∗) < 0 or
F(τ ∗, q∗) = 0. In such way the proof of the theorem is reduced to proving
the following four propositions:
Proposition 1. If the strategy q0 is Bayessian then F(τ ∗, q∗) = 0.
Proposition 2. If F(τ ∗, q∗) = 0 then the strategy q0 is Bayessian.
Proposition 3. If the strategy q0 is improper then F(τ ∗, q∗) < 0.
Proposition 4. If F(τ ∗, q∗) < 0 then the strategy q0 is improper.
Proof of Proposition 1. If the strategy q0 is Bayessian then according to
Deﬁnition 6 a weight function τ 0 exists such that inequality
X
θ∈Θ
τ 0(θ)R(q, θ) ≥
X
θ∈Θ
τ 0(θ)R(q0, θ)
is valid for all q ∈Q. Consequently, for all q ∈Q the chain
0 ≤
X
θ∈Θ
τ 0(θ)[R(q, θ) −R(q0, θ)] = F(τ 0, q) ≤max
τ∈T F(τ, q)
8

is also valid. Since all numbers maxτ∈T F(τ, q), q ∈Q, are not negative the
least of them is also not negative and
min
q∈Q max
τ∈T F(τ, q) = F(τ ∗, q∗) ≥0
From this inequality it follows that F(τ ∗, q∗) = 0 because a case F(τ ∗, q∗) > 0
is impossible.
Proof of Proposition 2. Let F(τ ∗, q∗) = 0 then
0 = F(τ ∗, q∗) = max
τ∈T min
q∈Q F(τ, q) = min
q∈Q F(τ ∗, q) =
= min
q∈Q
X
θ∈Θ
τ ∗(θ)

R(q, θ) −R(q0, θ)

=
= min
q∈Q
 X
θ∈Θ
τ ∗(θ)R(q, θ)

−
X
θ∈Θ
τ ∗(θ)R(q0, θ).
It implies the equality
min
q∈Q
X
θ∈Θ
τ ∗(θ)R(q, θ) =
X
θ∈Θ
τ ∗(θ)R(q0, θ)
and therefore,
q0 = arg min
q∈Q
X
θ∈Θ
τ ∗(θ)R(q, θ),
which means that q0 is Bayesian according to Deﬁnition 6.
Proof of Proposition 3.
If the strategy q0 is improper then according to
Deﬁnition 5 a strategy q1 exists such that inequality R(q1, θ) < R(q0, θ)
holds for all θ. The set of models is ﬁnite and therefore, a value ε < 0 exists
such that for any θ inequality R(q1, θ) −R(q0, θ) ≤ε holds and a chain
0 > ε ≥
X
θ∈Θ
τ(θ)[R(q1, θ) −R(q0, θ)] = F(τ, q1) ≥min
q∈Q F(τ, q)
is valid for any τ ∈T. Since all numbers minq∈Q F(τ, q), τ ∈T, are not
greater then ε the greatest of them is also not greater then ε and
max
τ∈T min
q∈Q F(τ, q) = F(τ ∗, q∗) ≤ε < 0.
9

Proof of Proposition 4. Let F(τ ∗, q∗) < 0 then
F(τ ∗, q∗) = min
q∈Q max
τ∈T F(τ, q) = max
τ∈T F(τ, q∗) =
= max
τ∈T
X
θ∈Θ
τ(θ)

R(q∗, θ) −R(q0, θ)

= max
θ∈Θ

R(q∗, θ) −R(q0, θ)

and therefore
max
θ∈Θ

R(q∗, θ) −R(q0, θ)

< 0.
Consequently, the inequality R(q∗, θ) < R(q0, θ) holds for all models θ ∈Θ
and q0 is improper according to Deﬁnition 5.
The theorem gives good reasons to reappraise lot of well-known methods
that are commonly used as something self-evident.
Let us illustrate this
criticism with two simple examples. The ﬁrst example considers a certain
method of recognition without learning and the second relates to maximum
likelihood learning. In both examples the loss function is
ω(y, y′) =
(
0,
if y = y′,
1,
if y ̸= y′.
Example 3. Let x be an image of a letter, y be its name and θ be a position
of the letter in a ﬁeld of vision. Let the function pXY : X × Y × Θ →R be
constructively deﬁned so that probability pXY (x, y; θ) can be calculated for
each triple x, y, θ. In this case when an image x with an unknown position
θ is observed the decision y∗(x) about the name of the letter has to be of the
form
y∗(x) = argmax
y∈Y
X
θ∈Θ
τ(θ)pXY (x, y; θ).
(1)
Theorem 1 reveals a certain weakness of the commonly used form
argmax
y∈Y
max
θ∈Θ pXY (x, y; θ).
(2)
The strategy (2) could be represented in the form (1) if the weights τ(θ) in
(1) could be chosen individually for each observation x ∈X. However, each
10

Bayessian strategy is speciﬁed with its own weight function τ : Θ →R so
that weights are assigned to elements of the set Θ, not of the set Θ × X.
As a rule, the strategy (2) cannot be represented in the form (1) with ﬁxed
weights τ(θ) that do not depend on x. It means that the strategy (2) is not
Bayessian and is dominated by some other strategy that for each position of
the letter recognizes its name better then strategy (2).
Example 4. Let the sets X, Y and Θ be speciﬁed for the recognized object
as well as a function pXY : X × Y × Θ →R. Let the learning information
be a random learning sample z = ((xi, yi)|i = 1, 2, . . . , n) such that
pZ(z; θ) =
n
Y
i=1
pXY (xi, yi; θ).
Then the decision y∗about the current state y0 based on the current signal
x0 and available learning sample z has to be of the form
y∗= arg max
y0∈Y
X
θ∈Θ
τ(θ)
n
Y
i=0
p(xi, yi; θ)
(3)
for some ﬁxed τ that does not depend on z. One can see that the commonly
used maximum likelihood strategy
y∗= arg max
y0 p(x0, y0; θML(z)),
(4)
θML(z) = arg max
θ∈Θ
n
Y
i=1
p(xi, yi; θ)
can almost never be represented in the form (3) with constant weights and
therefore is not Bayessian. It means that some other strategy exists that
makes a decision about the current state based both on current signal and
learning information and for each model makes it better than strategy (4).
4
A gap between maximum likelihood and
minimax strategies.
We consider maximum likelihood and minimax strategies and specify a gap
between them.
11

Let us deﬁne for each θ ∈Θ a strategy qopt(θ) = argminq∈Q R(q, θ) that
assigns a probability qopt(y|x, z; θ) for each triple (x, y, z).
The strategy
qopt(θ) is the best possible strategy that should be used if a true model were
known. Since the model is known no learning data are needed. For any ﬁxed
model θ a strategy q(θ) : X × Y × Z →R can be replaced with a strategy
qX(θ) : X × Y →R with the same risk. Probabilities q(y|x, z; θ) have to be
transformed into probabilities qX(y|x; θ) according to expression
qX(y|x; θ) =
X
z∈Z
pZ(z; θ)q(y|x, z; θ)
and so the chain
R(q, θ) =
X
z∈Z
X
x∈X
X
y∈Y
pXY (x, y; θ)pZ(z; θ)
X
y′∈Y
q(y′|x, z; θ)ω(y, y′) =
=
X
x∈X
X
y∈Y
pXY (x, y; θ)
X
y′∈Y
ω(y, y′)
X
z∈Z
pZ(z; θ)q(y′|x, z; θ) =
=
X
x∈X
X
y∈Y
pXY (x, y; θ)
X
y′∈Y
qX(y′|x; θ)ω(y, y′) = R(qX, θ).
is valid for each model θ. Consequently, for each θ the equality
min
q∈Q R(q, θ) = min
qX∈QX R(qX, θ)
(5)
is valid. The symbol QX in (5) designates a set of all strategies of the form
qX : X × Y →R that do not use the learning data.
Deﬁnition 7. A strategy qML : X × Y × Z →R is called a maximum
likelihood strategy if for each triple (x, y, z) it speciﬁes a probability
qML(y|x, z) = qopt
X (x|y; θML(z)),
where qopt
X (θ) = argmin
qX∈QX
R(qX, θ) and θML(z) = argmax
θ∈Θ
pZ(z; θ).
In other words, maximum likelihood strategies use the learning data z to
estimate a model θ and make a decision that minimizes the expected loss
with an assumption that the estimated model is the true model.
12

As it has been quoted for Examples 3 and 4, as a rule, maximum likelihood
strategies cannot be represented in a form of a Bayessian strategy
qB = argmin
q∈Q
X
θ∈Θ
τ(θ)R(q, θ)
with ﬁxed weights τ(θ) that do not depend on the learning data. In such
cases the maximum likelihood strategy qML may be dominated with another
strategy of the form X × Y × Z →R. Minimax strategies are free of this
ﬂaw.
Deﬁnition 8. Strategy argmin
q∈Q
max
θ∈Θ R(q, θ) is called a minimax strategy.
Theorem 2. No minimax strategy is improper.
Proof. Let us prove an equivalent statement that any improper strategy q0 is
not minimax. Indeed, as far as q0 is improper another strategy q1 exists such
that R(q1, θ) < R(q0, θ) for all θ. Therefore, maxθ R(q1, θ) < maxθ R(q0, θ)
and minq maxθ R(q, θ) < maxθ R(q0, θ) and q0 is not argminq maxθ R(q0, θ).
Though maximum likelihood strategy may be improper whereas minimax
strategy is never improper the ﬁrst one has an essential advantage over the
second. There is a rather wide class of learning data such that the maximum
likelihood strategy is in a sense consistent for any recognized object whereas
there is a rather wide class of recognized objects such that the minimax strat-
egy is not consistent for any learning data. Let us exactly formulate these
statements and prove them.
Let z ∈Z be a random variable that depends on model θ and let for each
z ∈Z and θ ∈Θ a probabillity pZ(z; θ) be given. We will say that this
dependence is essential if for each two diﬀerent models θ1 ̸= θ2 a value z∗
exists such that pZ(z∗; θ1) ̸= pZ(z∗; θ2). Let zn = (zi|i = 1, 2, . . . , n) ∈Zn be
a learning sample, pZn(zn; θ∗) = Qn
i=1 pZ(zi; θ∗) be a probability of a sample
and θML(zn) = argmaxθ pZn(zn; θ) be a maximum likelihood estimation of
the model.
Consistency is a generally known property of maximum likelihood esti-
mate. In the considered case this property may be formulated in a simple
13

way that the probability of inequality θML(zn) ̸= θ∗converges to zero when
n increases or, formally,
lim
n→∞
X
zn∈Zn
err
n
Y
i=1
pZ(zi; θ∗) = 0
(6)
where
Zn
err = {zn ∈Zn|θML(zn) ̸= θ∗}.
(7)
The consistency of a maximum likelihood estimations is a base for a proof of
the following theorem about consistency of maximum likelihood strategy.
Theorem 3. Let z be random variable that takes values from a set Z ac-
cording to probability distribution pZ(z; θ) that essentially depends on θ;
let n be a positive integer and zn = (zi|i = 1, 2, . . . , n) ∈Zn be a random
learning sample with probability distribution pZn(zn; θ) = Qn
i=1 pZ(zi; θ);
let qML
n
: X × Y × Zn →R be a maximum likelihood strategy for an object
⟨X, Y, Θ, pXY : X × Y × Θ →R⟩and learning data ⟨Zn, pZn : Zn × Θ →R⟩.
Then
lim
n→∞max
θ∈Θ

R(qML
n
, θ) −min
q∈Q R(q, θ)

= 0.
Proof. As far as a set Θ is ﬁnite the proof of the theorem is reduced to proof
of the equality
lim
n→∞

R(qML
n
, θ) −min
q∈Q R(q, θ)

= 0
(8)
for any θ. The subsequent proof is based on equality (5), on equalities (6)
and (7) that express consistency of maximum likelihood estimates and on
equality
R(qML
n
, θ) =
X
zn∈Zn
pZn(zn; θ) min
qX∈QX R(qX, θML(zn)),
where θML(zn) = argmax
θ∈Θ
pZn(zn; θ),
14

that follows from Deﬁnition 7. The following chain is valid:
lim
n→∞[R(qML
n
, θ) −min
q∈Q R(q, θ)] = lim
n→∞[R(qML
n
, θ) −min
qX∈QX R(qX, θ)] =
= lim
n→∞[
X
zn∈Zn
pZn(zn; θ) min
qX∈QX R(qX, θML(zn)) −min
qX∈QX R(qX, θ)]
= lim
n→∞
X
zn∈Zn
pZn(zn; θ)[ min
qX∈QX R(qX, θML(zn)) −min
qX∈QX R(qX, θ)]
= lim
n→∞
X
zn∈Zn
err
pZn(zn; θ)[ min
qX∈QX R(qX, θML(zn)) −min
qX∈QX RX(qX, θ)]
≤lim
n→∞
X
zn∈Zn
err
pZn(zn; θ)[max
y∈Y max
y′∈Y w(y, y′) −min
y∈Y min
y′∈Y w(y, y′)]
= lim
n→∞{[max
y∈Y max
y′∈Y w(y, y′) −min
y∈Y min
y′∈Y w(y, y′)]
X
zn∈Zn
err
pZn(zn; θ)}
= [max
y∈Y max
y′∈Y w(y, y′) −min
y∈Y min
y′∈Y w(y, y′)] lim
n→∞
X
zn∈Zn
err
pZn(zn; θ) = 0.
It follows from a chain that for any θ an inequality
lim
n→∞

R(qML
n
, θ) −min
q∈Q R(q, θ)

≤0
holds. The diﬀerence R(qML
n
, θ)−minq∈Q R(q, θ) is never negative and so (8)
is proved.
So, with the increasing length of learning sample the risk function of max-
imum likelihood strategy becomes arbitrarily close to the minimum possible
risk function. Minimax strategy has not this property. Moreover, for certain
class of objects minimax strategies simply ignore the learning sample, no
matter how long it is.
Theorem 4. Let for an object ⟨X, Y, Θ, pXY : X × Y × Θ →R⟩a pair (θ∗, q∗
X)
exists such that
q∗
X = argmin
qX∈QX
R(qX, θ∗),
θ∗= argmax
θ∈Θ
R(q∗
X, θ).
Then the inequality
max
θ∈Θ R(q, θ) ≥max
θ∈Θ R(q∗
X, θ)
(9)
is valid for any learning data ⟨Z, pZ : Z × Θ →R⟩and any strategy q : X ×
Y × Z →R.
15

Proof. For any strategy q ∈Q we have the chain
max
θ∈Θ R(q, θ) ≥R(q, θ∗) ≥min
q∈Q R(q, θ∗) =
= min
qX∈QX R(qX, θ∗) = R(q∗
X, θ∗) = max
θ∈Θ R(q∗
X, θ).
The theorem shows that for some objects the minimax approach is partic-
ularly inappropriate because it enforces to ignore any learning data. There
is nothing unusual in conditions of the Theorem 4. Examples 1 and 2 in
Introduction show just the cases when these conditions are satisﬁed.
So, there is a following gap between maximum likelihood and minimax strate-
gies. Maximum likelihood strategy may be dominated with other strategy.
In this case it can be improved and, consequently, it is not optimal from any
point of view. However, for wide class of learning data maximum likelihood
strategies are consistent and so their chortage does not become apparent
when learning sample of an arbitrary size may be obtained. Cases of learn-
ing samples of ﬁxed sizes, especially, short samples form an area of improper
application of maximum likelihood strategies. This area is not covered with
minimax strategies. Though minimax strategies are dominated with no strat-
egy, for rather wide class of objects minimax requirement enforces to ignore
any learning sample, no matter how long it is.
5
Minimax deviation strategies.
This section is aimed at developing a Bayesian consistent strategy that has
to ﬁll a gap between maximum likelihood and minimax strategies.
Deﬁnition 9. A strategy argmin
q∈Q
max
θ∈Θ

R(q, θ) −min
q′∈Q R(q′, θ)

is called mini-
max deviation strategy.
Minimax deviation strategies do not have the drawback of the minimax
strategies. A theorem that is similar to Theorem 3 for maximum likelihood
strategies is also valid for minimax deviation strategies.
16

Theorem 5. Let z be random variable that takes values from a set Z ac-
cording to probability distribution pZ(z; θ) that essentially depends on θ;
let n be a positive integer and zn = (zi|i = 1, 2, . . . , n) ∈Zn is a random
learning sample with probability distribution pZn(zn; θ) = Qn
i=1 pZ(zi; θ);
let q∗
n : X × Y × Zn →R be a minimax deviation strategy for an object
⟨X, Y, Θ, pXY : X × Y × Θ →R⟩and learning data ⟨Zn, pZn : Zn × Θ →R⟩.
Then
lim
n→∞max
θ∈Θ

R(q∗
n, θ) −min
q∈Q R(q, θ)

= 0.
(10)
Proof. The Theorem is a straighforward consequence of Deﬁnition 9 and
the Theorem 3. Let qML
n
be a maximum likelihood strategy for an object
⟨X, Y, Θ, pXY : X × Y × Θ →R⟩and learning data ⟨Zn, pZn : Zn × Θ →R⟩.
It follows from Deﬁnition 9 that
max
θ∈Θ

R(q∗
n, θ) −min
q∈Q R(q, θ)

≤max
θ∈Θ

R(qML
n
, θ) −min
q∈Q R(q, θ)

for any n. It follows from Theorem 3 that
lim
n→∞max
θ∈Θ

R(q∗
n, θ) −min
q∈Q R(q, θ)

≤
≤lim
n→∞max
θ∈Θ

R(qML
n
, θ) −min
q∈Q R(q, θ)

= 0.
As far as the diﬀerence [R(q∗
n, θ) −minq∈Q R(q, θ)

is negative for no model
the equality (10) is proved.
Let us note that the proof of the Theorem 10 shows not only a consistency
of minimax deviation strategy. It shows also that minimax deviation strat-
egy converges to desired result not slower than maximum likelihood strategy.
Similarly, one can show that this advantage of minimax deviation strategy
holds as compared with any consistent strategy and from this point of view
it is the best of all consistent strategies.
Following theorem states that minimax deviation strategies are also inap-
propriate for recognition of certain type of objects.
17

Theorem 6. Let for an object ⟨X, Y, Θ, p : X × Y × Θ →R⟩a model θ∗and
a strategy q∗
X exist such that
q∗
X = argmin
qX∈QX
[RX(qX, θ∗) −min
q′
X∈QX RX(q′
X, θ∗)],
(11)
θ∗= argmax
θ∈Θ
[RX(q∗
X, θ) −min
q′
X∈QX RX(q′
X, θ)].
(12)
Then the inequality
max
θ∈Θ [R(q, θ) −min
qX∈QX R(qX, θ)] ≥max
θ∈Θ [R(q∗
X, θ) −min
qX∈QX R(qX, θ)]
holds for any learning data ⟨Z, pZ : Z × Θ →R⟩and any strategy q ∈Q.
Proof. In fact, proof of the theorem does not diﬀer from the proof of the
Theorem 4.
However, the consequences of this theorem for minimax deviation strate-
gies are not so destructive as those of Theorem 4 for minimax strategies. In
fact, conditions (11) and (12) imply that a strategy q∗
X ∈QX exists that
does not use learning information and assures minimal possible risk for each
model,
R(q∗
X, θ) = min
qX∈QX R(qX, θ) for all θ ∈Θ.
In this case, any learning data are needless and has to be omitted by any
strategy.
Evidently, minimax deviation strategy is not improper and, consequently,
is Bayessian. The following theorem shows how the corresponding weight
function has to be obtained.
Theorem 7. Minimax deviation strategy
q∗= argmin
q∈Q
max
θ∈Θ

R(q, θ) −min
qX∈QX R(qX, θ)

is a Bayesian strategy argmin
q∈Q
P
θ∈Θ
τ ∗(θ)R(q, θ) with respect to weight function
τ ∗= arg max
τ∈T
"
min
q∈Q
X
θ∈Θ
τ(θ)R(q, θ) −
X
θ∈Θ
τ(θ) min
qX∈QX R(qX, θ)
#
.
(13)
18

Proof. Let us deﬁne a function F : T × Q →R,
F(τ, q) =
X
θ∈Θ
τ(θ)R(q, θ) −
X
θ∈Θ
τ(θ) min
qX∈QX R(qX, θ)
and express q∗and τ ∗in terms of F,
q∗= argmin
q∈Q
max
θ∈Θ

R(q, θ) −min
qX∈QX R(qX, θ)

= argmin
q∈Q
max
τ∈T
X
θ∈Θ
τ(θ)

R(q, θ) −min
qX∈QX R(qX, θ)

= argmin
q∈Q
max
τ∈T F(τ, q),
τ ∗= arg max
τ∈T min
q∈Q F(τ, q).
The function F is a linear function of q for ﬁxed τ and a linear function of
τ for ﬁxed q and is deﬁned on a Cartesian product of two closed convex sets
T and Q. In such case a pair (τ ∗, q∗) is a saddle point [1, 2, 4],
min
q∈Q max
τ∈T F(τ, q) = F(τ ∗, q∗) = max
τ∈T min
q∈Q F(τ, q),
that implies F(τ ∗, q∗) = min
q∈Q F(τ ∗, q) and
q∗= arg min
q∈Q F(τ ∗, q) =
= arg min
q∈Q
"X
θ∈Θ
τ ∗(θ)R(q, θ) −
X
θ∈Θ
τ ∗(θ) min
qX∈QX R(qX, θ)
#
=
= arg min
q∈Q
X
θ∈Θ
τ ∗(θ)R(q, θ).
In such way developing minimax deviation strategy is reduced to calculat-
ing weights τ(θ) of models that maximize concave function (13). In described
below experiments general purpose methods of non-smooth optimization [6]
were used.
19

6
Experiments
Minimax deviation strategies have been built for objects considered in In-
troduction in Examples 1 and 2. Minimax deviation strategies have been
compared with maximum likelihood and minimax strategies.
Results are
presented on Figures 5 and 6 that show risk R(q, θ) of the strategies as a
function of a model for several learning sample sizes. Figure 5 relates to
Example 1 and Figure 6 to Example 2.
θ
b
−6
−3
−0
3
6
b
b
b
b
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
θ
b
−6
−3
−0
3
6
b
b
b
b
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
n = 1
n = 2
θ
b
−6
−3
−0
3
6
b
b
b
b
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
θ
b
−6
−3
−0
3
6
b
b
b
b
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
n = 3
n = 10
Figure 5: Example 1. Probability of making a wrong decision for diﬀerent
sizes n of the learning sample. The dashed line shows the risk of a minimax
deviation strategy. The curve R(qML, θ) is the risk of a maximum likelihood
strategy. The curve R(qminmax, θ) is the risk of a minimax strategy. The
curve min
q
R(q, θ) is the minimum possible risk for each model.
7
Conclusion
The paper analyzes the problem when for given object

X, Y, Θ, pXY : X × Y × Θ →R

,
loss function w : Y × Y →R, learning data source

Z, pZ : Z × Θ →R

,
observed current signal x and available learning data z a decision y∗about
20

θ
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
b0
b0.5
b1
θ
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
b0
b0.5
b1
n = 1
n = 2
θ
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
b0
b0.5
b1
θ
R(q, θ)
min
q
R(q, θ)
R(qmin max, θ)
R(qML, θ)
b0
b0.5
b1
n = 5
n = 10
Figure 6: Example 2. Probability of making a wrong decision for diﬀerent
sizes n of the learning sample. The dashed line shows the risk of a minimax
deviation strategy. The curve R(qML, θ) is the risk of a maximum likelihood
strategy. The curve R(qminmax, θ) is the risk of a minimax strategy. The
curve min
q
R(q, θ) is the minimum possible risk for each model.
current hidden state y has to be made. The wide class of commonly used
strategies make the decision of a form
y∗= argmin
y′∈Y
X
y∈Y
pXY (x, y; θest(z))w(y, y′)
(14)
where θest : Z →Θ is a reasonable estimating a model θ based on learning
data z. It means that the learning data are used to choose a single best model
and the objects are recognized as if this best model equals the true model.
The approach is acceptable if learning data are arbitrarily long learning sam-
ples and estimator θest : Z →Θ is consistent. If the learning information has
a ﬁxed format, for example, is a learning sample of limited size then the ap-
proach gives no guarantee for subsequent recognition. Indeed, the approach
is not deduced from any risk-oriented requirement. Reasonable requirement
21

to the quality of post-learning recognition implies the decision of the form
y∗= argmin
y′∈Y
X
θ∈Θ
τ(θ)pZ(z; θ)
X
y∈Y
pXY (x, y; θ)w(y, y′)
(15)
that diﬀers from (14). Moreover, any decision that diﬀers from (15) can be
replaced with a decision of the form (15) with the better recognition quality.
There is nothing in decision (15) that could be treated as a selecting some best
model of the model set and so no question stands what estimator θest : Z →Θ
has to be used. No model has to be selected, on the contrary, all models have
to take part in decision with their weights. It is essential that the weights do
not depend on learning data, they are determined by requirement to searched
strategy for concrete applied situation. The paper shows a way for computing
these weights for minimax deviation strategy that is appropriate for learning
samples of any length and in such way ﬁlls a gap between maximum likeli-
hood and minimax startegies.
Minimax deviation strategy is not at all a single strategy that is reason-
able in such or other application. Many other strategies are appropriate too,
for example, a strategies of the form
argmin
q∈Q
max
θ∈Θ
R(q, θ) −α(θ)
β(θ)
(16)
with predeﬁned numbers α(θ) and β(θ) > 0. Minimax strategy is a special
case of (16) when α(θ) = 0, β(θ) = 1, minimax deviation strategy is a case
when α(θ) = minq∈Q R(q, θ), β(θ) = 1. A reasonable modiﬁcation of mini-
max deviation strategy is a case when α(θ) = 0, β(θ) = minq∈Q R(q, θ). The
numbers α(θ) may be risks of some already developed strategy and this is a
case when the developer wants to check whether the better strategy is possi-
ble. At last, numbers α(θ) may be simply desired values of risks in concrete
applied situation.
Requirements of the form (16) together with various loss functions deter-
mine various applied situations and obtained results show the way to cope
with all them. It has become quite clear now that each strategy of the form
(16) may be represented in the form (15) because, obviously, no of them is
improper. Obtained results imply unexpected conclusion that learning data
22

take part in a decision (15) in a uniﬁed form that depends neither on applied
situation nor on recognized object. So, no question stands more how the
learning data have to inﬂuence the decision about current state when the
current signal is observed. Learning data inﬂuence the decision via and only
via probabilities pZ; (z; θ), not via choise of some best model of the model
set.
References
[1] J.M. Borwein and A.S. Lewis. Convex Analysis and Nonlinear Optimiza-
tion. Springer Verlag, 2000.
[2] S. Boyd and L. Vandenberghe. Convex Optimization. Cambrige Univer-
sity Press, 2004.
[3] Richard O. Duda, Peter E. Hart, and David G. Stork. Pattern Classiﬁ-
cation. Wiley, 2000.
[4] J.-B. Hiriart-Urruty and C. Lemarechal. Fundamentals of Convex Anal-
ysis. Springer Verlag, 2002.
[5] Herbert Robbins. Asymptotically Subminimax Solutions of Compound
Statistical Decision Problems. In Jerzy Neyman, editor, Proceedings of the
Second Berkeley Symposium on Mathematical Statistics and Probability,
pages 131–148. University of California Press, 1951.
[6] N.Z. Shor.
Nondiﬀerentiable Optimization and Polynomial Problems.
Nonconvex Optimization and Its Applications. Springer, 1998.
[7] Andrew R. Webb. Statistical Pattern Recognition. Wiley, 2002.
23